{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of pytorch BERT\n",
    "\n",
    "## add some preprocessing and use simple pytorch BERT \n",
    "\n",
    "- next step, I will try customize BERT model that I re-write BERT module.\n",
    "\n",
    "\n",
    "**BTW: I train model in offline, and upload to inference is always scoring error... :(**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# !pip install ../input/sacremoses/sacremoses-master/\n",
    "# !pip install ../input/transformers/transformers-master/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('../input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.models import Word2Vec\n",
    "from flashtext import KeywordProcessor\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
    "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from math import floor, ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTS = {\n",
    "            '》', '〞', '¢', '‹', '╦', '║', '♪', 'Ø', '╩', '\\\\', '★', '＋', 'ï', '<', '?', '％', '+', '„', 'α', '*', '〰', '｟', '¹', '●', '〗', ']', '▾', '■', '〙', '↓', '´', '【', 'ᴵ',\n",
    "            '\"', '）', '｀', '│', '¤', '²', '‡', '¿', '–', '」', '╔', '〾', '%', '¾', '←', '〔', '＿', '’', '-', ':', '‧', '｛', 'β', '（', '─', 'à', 'â', '､', '•', '；', '☆', '／', 'π',\n",
    "            'é', '╗', '＾', '▪', ',', '►', '/', '〚', '¶', '♦', '™', '}', '″', '＂', '『', '▬', '±', '«', '“', '÷', '×', '^', '!', '╣', '▲', '・', '░', '′', '〝', '‛', '√', ';', '】', '▼',\n",
    "            '.', '~', '`', '。', 'ə', '］', '，', '{', '～', '！', '†', '‘', '﹏', '═', '｣', '〕', '〜', '＼', '▒', '＄', '♥', '〛', '≤', '∞', '_', '[', '＆', '→', '»', '－', '＝', '§', '⋅', \n",
    "            '▓', '&', 'Â', '＞', '〃', '|', '¦', '—', '╚', '〖', '―', '¸', '³', '®', '｠', '¨', '‟', '＊', '£', '#', 'Ã', \"'\", '▀', '·', '？', '、', '█', '”', '＃', '⊕', '=', '〟', '½', '』',\n",
    "            '［', '$', ')', 'θ', '@', '›', '＠', '｝', '¬', '…', '¼', '：', '¥', '❤', '€', '−', '＜', '(', '〘', '▄', '＇', '>', '₤', '₹', '∅', 'è', '〿', '「', '©', '｢', '∙', '°', '｜', '¡', \n",
    "            '↑', 'º', '¯', '♫', '#'\n",
    "          }\n",
    "\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n",
    "\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n",
    "\n",
    "\n",
    "def clean_punct(text):\n",
    "  text = str(text)\n",
    "  for punct in PUNCTS:\n",
    "    text = text.replace(punct, ' {} '.format(punct))\n",
    "  \n",
    "  return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = KeywordProcessor(case_sensitive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in mispell_dict.items():\n",
    "    kp.add_keyword(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\&lt)|(\\&gt)', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', text)\n",
    "    text = kp.replace_keywords(text)\n",
    "    text = clean_punct(text)\n",
    "    text = re.sub(r'\\n\\r', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_title'] = train['question_title'].apply(lambda x : preprocessing(x))\n",
    "train['clean_body'] = train['question_body'].apply(lambda x : preprocessing(x))\n",
    "train['clean_answer'] = train['answer'].apply(lambda x : preprocessing(x))\n",
    "\n",
    "test['clean_title'] = test['question_title'].apply(lambda x : preprocessing(x))\n",
    "test['clean_body'] = test['question_body'].apply(lambda x : preprocessing(x))\n",
    "test['clean_answer'] = test['answer'].apply(lambda x : preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = ['question_asker_intent_understanding',\n",
    "       'question_body_critical', 'question_conversational',\n",
    "       'question_expect_short_answer', 'question_fact_seeking',\n",
    "       'question_has_commonly_accepted_answer',\n",
    "       'question_interestingness_others', 'question_interestingness_self',\n",
    "       'question_multi_intent', 'question_not_really_a_question',\n",
    "       'question_opinion_seeking', 'question_type_choice',\n",
    "       'question_type_compare', 'question_type_consequence',\n",
    "       'question_type_definition', 'question_type_entity',\n",
    "       'question_type_instructions', 'question_type_procedure',\n",
    "       'question_type_reason_explanation', 'question_type_spelling',\n",
    "       'question_well_written', 'answer_helpful',\n",
    "       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "       'answer_satisfaction', 'answer_type_instructions',\n",
    "       'answer_type_procedure', 'answer_type_reason_explanation',\n",
    "       'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"../input/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_input(tokenizer, title, body, answer, max_seq_length=512):\n",
    "    all_title = []\n",
    "    all_body = []\n",
    "    all_answer = []\n",
    "    for t, b, a in tqdm(zip(title, body, answer), total=len(title)):\n",
    "        \n",
    "        tokenizer_t = tokenizer.tokenize(t)\n",
    "        tokenizer_b = tokenizer.tokenize(b)\n",
    "        tokenizer_a = tokenizer.tokenize(a)\n",
    "        \n",
    "        t_len = len(tokenizer_t)\n",
    "        b_len = len(tokenizer_b)\n",
    "        a_len = len(tokenizer_a)\n",
    "        \n",
    "        t_max_len=TITLE_MAX_LEN\n",
    "        b_max_len=BODY_MAX_LEN\n",
    "        a_max_len=ANSWER_MAX_LEN\n",
    "        \n",
    "        if (t_len+b_len+a_len+4) > max_seq_length:\n",
    "        \n",
    "            if t_max_len > t_len:\n",
    "                t_new_len = t_len\n",
    "                a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
    "                b_max_len = b_max_len + ceil((t_max_len - t_len)/2)\n",
    "            else:\n",
    "                t_new_len = t_max_len\n",
    "\n",
    "            if a_max_len > a_len:\n",
    "                a_new_len = a_len \n",
    "                b_new_len = b_max_len + (a_max_len - a_len)\n",
    "            elif b_max_len > b_len:\n",
    "                a_new_len = a_max_len + (b_max_len - b_len)\n",
    "                b_new_len = b_len\n",
    "            else:\n",
    "                a_new_len = a_max_len\n",
    "                b_new_len = b_max_len\n",
    "\n",
    "\n",
    "            if t_new_len+a_new_len+b_new_len+4 != max_seq_length:\n",
    "                raise ValueError(\"New sequence length should be %d, but is %d\"%(max_seq_length, (t_new_len + a_new_len + b_new_len + 4)))\n",
    "\n",
    "\n",
    "\n",
    "            tokenizer_t = tokenizer_t[:t_new_len]\n",
    "            tokenizer_b = tokenizer_b[:b_new_len]\n",
    "            tokenizer_a = tokenizer_a[:a_new_len]\n",
    "\n",
    "        all_title.append(tokenizer_t)\n",
    "        all_body.append(tokenizer_b)\n",
    "        all_answer.append(tokenizer_a)\n",
    "        \n",
    "    return all_title, all_body, all_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6079/6079 [01:08<00:00, 88.37it/s]\n",
      "100%|██████████| 6079/6079 [00:03<00:00, 1529.04it/s]\n",
      "100%|██████████| 476/476 [00:05<00:00, 82.82it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 1314.78it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    \n",
    "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = get_ids(stoken, tokenizer, max_sequence_length)\n",
    "    input_masks = get_masks(stoken, max_sequence_length)\n",
    "    input_segments = get_segments(stoken, max_sequence_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]\n",
    "\n",
    "def convert_lines(tokenizer, title, body, answer, max_seq_length=512):\n",
    "    title, body, answer = trim_input(tokenizer, title, body, answer)\n",
    "#     max_seq_length -= 4\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    \n",
    "    all_tokens = []\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for i, (t, b, a) in tqdm(enumerate(zip(title, body, answer)), total=len(title)):\n",
    "        stoken = [\"[CLS]\"] + t + [\"[SEP]\"] + b + [\"[SEP]\"] + a + [\"[SEP]\"]\n",
    "        \n",
    "        ids = get_ids(stoken, tokenizer, max_seq_length)\n",
    "        masks = get_masks(stoken, max_seq_length)\n",
    "        segments = get_segments(stoken, max_seq_length)\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "        \n",
    "    return [\n",
    "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "    ]\n",
    "\n",
    "TITLE_MAX_LEN = 30\n",
    "BODY_MAX_LEN = 239\n",
    "ANSWER_MAX_LEN = 239\n",
    "# title, body, answer = trim_input(tokenizer, train['clean_title'], train['clean_body'], train['clean_answer'])\n",
    "# title, body, answer = trim_input(tokenizer, test['clean_title'], test['clean_body'], test['clean_answer'])\n",
    "\n",
    "x_train = convert_lines(tokenizer, train['clean_title'], train['clean_body'], train['clean_answer'])\n",
    "x_test = convert_lines(tokenizer, test['clean_title'], test['clean_body'], test['clean_answer'])\n",
    "\n",
    "# x_train_body = convert_lines(tokenizer, train['clean_body'], BODY_MAX_LEN)\n",
    "# x_train_answer = convert_lines(tokenizer, train['clean_answer'], ANSWER_MAX_LEN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.TensorDataset):\n",
    "\n",
    "    def __init__(self, x_train, idxs, targets=None):\n",
    "        self.input_ids = x_train[0][idxs]\n",
    "        self.input_masks = x_train[1][idxs]\n",
    "        self.input_segments = x_train[2][idxs]\n",
    "        self.targets = targets[idxs] if targets is not None else np.zeros((x_train[0].shape[0], 30))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids =  self.input_ids[idx]\n",
    "        input_masks = self.input_masks[idx]\n",
    "        input_segments = self.input_segments[idx]\n",
    "\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        return input_ids, input_masks, input_segments, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 1\n",
    "\n",
    "SEED = 2020\n",
    "NFOLDS = 3\n",
    "BATCH_SIZE = 6\n",
    "EPOCHS = 4\n",
    "LR = 3e-5\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_config = '../input/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "bert_config.num_labels = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "Epoch 1/4 \t loss=0.4330 \t val_loss=0.3939 \t spearman=0.27 \t time=287.73s\n",
      "Epoch 2/4 \t loss=0.3875 \t val_loss=0.3775 \t spearman=0.33 \t time=286.72s\n",
      "Epoch 3/4 \t loss=0.3715 \t val_loss=0.3720 \t spearman=0.36 \t time=286.81s\n",
      "Epoch 4/4 \t loss=0.3613 \t val_loss=0.3709 \t spearman=0.36 \t time=286.66s\n",
      "fold 2\n",
      "Epoch 1/4 \t loss=0.4349 \t val_loss=0.3957 \t spearman=0.26 \t time=286.73s\n",
      "Epoch 2/4 \t loss=0.3876 \t val_loss=0.3773 \t spearman=0.33 \t time=286.77s\n",
      "Epoch 3/4 \t loss=0.3721 \t val_loss=0.3737 \t spearman=0.34 \t time=286.67s\n",
      "Epoch 4/4 \t loss=0.3616 \t val_loss=0.3699 \t spearman=0.36 \t time=286.87s\n",
      "fold 3\n",
      "Epoch 1/4 \t loss=0.4385 \t val_loss=0.3968 \t spearman=0.27 \t time=286.68s\n",
      "Epoch 2/4 \t loss=0.3864 \t val_loss=0.3786 \t spearman=0.34 \t time=286.90s\n",
      "Epoch 3/4 \t loss=0.3704 \t val_loss=0.3740 \t spearman=0.36 \t time=286.59s\n",
      "Epoch 4/4 \t loss=0.3597 \t val_loss=0.3704 \t spearman=0.37 \t time=286.70s\n"
     ]
    }
   ],
   "source": [
    "y = train.loc[:, y_columns].values\n",
    "\n",
    "oof = np.zeros((len(train), 30))\n",
    "test_pred = np.zeros((len(test), 30))\n",
    "\n",
    "\n",
    "kf = list(KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED).split(train))\n",
    "k = 0\n",
    "for i, (train_idx, valid_idx) in enumerate(kf):\n",
    "    print(f'fold {i+1}')\n",
    "    gc.collect()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(TextDataset(x_train, train_idx, y),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(TextDataset(x_train, valid_idx, y),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    net = BertForSequenceClassification.from_pretrained('../input/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config)\n",
    "    \n",
    "    net.cuda()\n",
    "    \n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = AdamW(net.parameters(), lr=LR, eps=4e-5)\n",
    "    \n",
    "    for epoch in range(EPOCHS):  \n",
    "        start_time = time.time()\n",
    "        avg_loss = 0.0\n",
    "        net.train()\n",
    "        for data in train_loader:\n",
    "\n",
    "            # get the inputs\n",
    "            input_ids, input_masks, input_segments, labels = data\n",
    "            pred = net(input_ids = input_ids.long().cuda(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks.cuda(),\n",
    "                             token_type_ids = input_segments.cuda(),\n",
    "                            )[0]\n",
    "            loss = loss_fn(pred, labels.cuda())\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            \n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = 0.0\n",
    "        net.eval()\n",
    "\n",
    "        valid_preds = np.zeros((len(valid_idx), 30))\n",
    "        true_label = np.zeros((len(valid_idx), 30))\n",
    "        for j, data in enumerate(val_loader):\n",
    "\n",
    "            # get the inputs\n",
    "#             body, answer, title, category, host, labels = data\n",
    "#             content, labels = data\n",
    "            input_ids, input_masks, input_segments, labels = data\n",
    "\n",
    "            ## forward + backward + optimize\n",
    "            pred = net(input_ids = input_ids.long().cuda(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks.cuda(),\n",
    "                             token_type_ids = input_segments.cuda(),\n",
    "                            )[0]\n",
    "            loss_val = loss_fn(pred, labels.cuda())\n",
    "            avg_val_loss += loss_val.item()\n",
    "\n",
    "            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n",
    "        \n",
    "        \n",
    "        score = 0\n",
    "        for i in range(30):\n",
    "            score += np.nan_to_num(\n",
    "                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation / 30)\n",
    "        oof[valid_idx] = valid_preds\n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t spearman={:.2f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, EPOCHS, avg_loss / len(train_loader), avg_val_loss / len(val_loader), score, elapsed_time))\n",
    "        \n",
    "    test_pred_fold = np.zeros((len(test), 30))\n",
    "    k += 0\n",
    "    torch.save(net.state_dict(), \"bert_pytorch_folds_{}.pt\".format(k))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for q, data in enumerate(test_loader):\n",
    "            input_ids, input_masks, input_segments, labels = data\n",
    "            y_pred = net(input_ids = input_ids.long().cuda(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks.cuda(),\n",
    "                             token_type_ids = input_segments.cuda(),\n",
    "                            )[0]\n",
    "            test_pred_fold[q * BATCH_SIZE:(q+1) * BATCH_SIZE] = torch.sigmoid(y_pred).cpu().detach().numpy()\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    test_pred += test_pred_fold/NFOLDS\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score = 0\n",
    "for i in range(30):\n",
    "    oof_score += np.nan_to_num(\n",
    "            spearmanr(y[:, i], oof[:, i]).correlation / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3606058150696535"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[:, y_columns] = test_pred\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.934227</td>\n",
       "      <td>0.662020</td>\n",
       "      <td>0.137694</td>\n",
       "      <td>0.594912</td>\n",
       "      <td>0.696661</td>\n",
       "      <td>0.642484</td>\n",
       "      <td>0.671438</td>\n",
       "      <td>0.604914</td>\n",
       "      <td>0.564129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904582</td>\n",
       "      <td>0.937408</td>\n",
       "      <td>0.654502</td>\n",
       "      <td>0.971258</td>\n",
       "      <td>0.971702</td>\n",
       "      <td>0.870977</td>\n",
       "      <td>0.091539</td>\n",
       "      <td>0.076258</td>\n",
       "      <td>0.904397</td>\n",
       "      <td>0.938939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.866979</td>\n",
       "      <td>0.536695</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.764195</td>\n",
       "      <td>0.811811</td>\n",
       "      <td>0.923753</td>\n",
       "      <td>0.550521</td>\n",
       "      <td>0.453628</td>\n",
       "      <td>0.065128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.703957</td>\n",
       "      <td>0.934021</td>\n",
       "      <td>0.637650</td>\n",
       "      <td>0.962830</td>\n",
       "      <td>0.973176</td>\n",
       "      <td>0.867537</td>\n",
       "      <td>0.908335</td>\n",
       "      <td>0.149921</td>\n",
       "      <td>0.089910</td>\n",
       "      <td>0.885434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.912635</td>\n",
       "      <td>0.648361</td>\n",
       "      <td>0.022647</td>\n",
       "      <td>0.768947</td>\n",
       "      <td>0.891633</td>\n",
       "      <td>0.922559</td>\n",
       "      <td>0.621986</td>\n",
       "      <td>0.518944</td>\n",
       "      <td>0.332015</td>\n",
       "      <td>...</td>\n",
       "      <td>0.861383</td>\n",
       "      <td>0.945991</td>\n",
       "      <td>0.664380</td>\n",
       "      <td>0.974129</td>\n",
       "      <td>0.981038</td>\n",
       "      <td>0.893448</td>\n",
       "      <td>0.118664</td>\n",
       "      <td>0.066873</td>\n",
       "      <td>0.893087</td>\n",
       "      <td>0.925513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.864932</td>\n",
       "      <td>0.473601</td>\n",
       "      <td>0.009179</td>\n",
       "      <td>0.701472</td>\n",
       "      <td>0.775042</td>\n",
       "      <td>0.906819</td>\n",
       "      <td>0.552135</td>\n",
       "      <td>0.423715</td>\n",
       "      <td>0.079881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.700547</td>\n",
       "      <td>0.940850</td>\n",
       "      <td>0.653747</td>\n",
       "      <td>0.967801</td>\n",
       "      <td>0.979457</td>\n",
       "      <td>0.877523</td>\n",
       "      <td>0.742106</td>\n",
       "      <td>0.146563</td>\n",
       "      <td>0.597580</td>\n",
       "      <td>0.902442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.882618</td>\n",
       "      <td>0.434423</td>\n",
       "      <td>0.014335</td>\n",
       "      <td>0.746622</td>\n",
       "      <td>0.799990</td>\n",
       "      <td>0.890851</td>\n",
       "      <td>0.589071</td>\n",
       "      <td>0.472883</td>\n",
       "      <td>0.143791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706801</td>\n",
       "      <td>0.936602</td>\n",
       "      <td>0.645741</td>\n",
       "      <td>0.967827</td>\n",
       "      <td>0.978169</td>\n",
       "      <td>0.863384</td>\n",
       "      <td>0.449051</td>\n",
       "      <td>0.125848</td>\n",
       "      <td>0.731147</td>\n",
       "      <td>0.913863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.934227                0.662020   \n",
       "1     46                             0.866979                0.536695   \n",
       "2     70                             0.912635                0.648361   \n",
       "3    132                             0.864932                0.473601   \n",
       "4    200                             0.882618                0.434423   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.137694                      0.594912   \n",
       "1                 0.005711                      0.764195   \n",
       "2                 0.022647                      0.768947   \n",
       "3                 0.009179                      0.701472   \n",
       "4                 0.014335                      0.746622   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.696661                               0.642484   \n",
       "1               0.811811                               0.923753   \n",
       "2               0.891633                               0.922559   \n",
       "3               0.775042                               0.906819   \n",
       "4               0.799990                               0.890851   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.671438                       0.604914   \n",
       "1                         0.550521                       0.453628   \n",
       "2                         0.621986                       0.518944   \n",
       "3                         0.552135                       0.423715   \n",
       "4                         0.589071                       0.472883   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.564129  ...               0.904582        0.937408   \n",
       "1               0.065128  ...               0.703957        0.934021   \n",
       "2               0.332015  ...               0.861383        0.945991   \n",
       "3               0.079881  ...               0.700547        0.940850   \n",
       "4               0.143791  ...               0.706801        0.936602   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.654502          0.971258          0.971702   \n",
       "1                     0.637650          0.962830          0.973176   \n",
       "2                     0.664380          0.974129          0.981038   \n",
       "3                     0.653747          0.967801          0.979457   \n",
       "4                     0.645741          0.967827          0.978169   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.870977                  0.091539               0.076258   \n",
       "1             0.867537                  0.908335               0.149921   \n",
       "2             0.893448                  0.118664               0.066873   \n",
       "3             0.877523                  0.742106               0.146563   \n",
       "4             0.863384                  0.449051               0.125848   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.904397             0.938939  \n",
       "1                        0.089910             0.885434  \n",
       "2                        0.893087             0.925513  \n",
       "3                        0.597580             0.902442  \n",
       "4                        0.731147             0.913863  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
